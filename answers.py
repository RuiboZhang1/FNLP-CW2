a1aa=['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']
a1b=2649
a1c=12.061525956381585
a1d='function'
a2a=13
a2b=2.4630366660416674
a3c=2.883894963080882
a3d='DET'
a4a3=0.8941143860297515
a4b1=[('``', '.'), ('My', 'DET'), ('taste', 'NOUN'), ('is', 'VERB'), ('gaudy', 'ADV'), ('.', '.')]
a4b2=[('``', '.'), ('My', 'DET'), ('taste', 'NOUN'), ('is', 'VERB'), ('gaudy', 'ADJ'), ('.', '.')]
a4b3="The word 'gaudy' might rare in the train data as ADJ. Therefore, self.elprob('ADJ', 'gaudy') is low. There might more VERB followed by ADV than ADJ in the training data. The probability of transition model self.tlprob('VERB', 'ADV') is higher than self.tlprob('VERB', 'ADJ'). \n   "
a4c=60.73342283709656
a4d=70.21922688308214
a4e=['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADV']
a5t0=0.558440358495796
a5tk=0.6090732698882011
a5b='The emission model for ‘he’ is the same for all states in the t0 since ‘he’ doesn’t appear in the training set. ‘he’ is tagged as ‘Num’ because of the bias transition model. After a few iterations, the transition model tlprob(‘VERB’, ‘PRON’)  and the emission model elprob(‘PRON’, ‘he’) might be higher. The last word ‘them’ is mislabeled by tk. Harm EM might overfit the data. tlprob(NUM, ‘ADP’) and tlprob(‘NOUN’, ‘.’) might be much larger than elprob(‘PRON’, ‘them’). Therefore, mistag the label.'
a6='We use the pos-tagger to tag the whole sentences and extract the tags for unseen words. We can calculate the emission probability from the tags to those words. For each unseen word, combine the emission probability of this word with the original probability of words in the same tags then normalise the probability.'
a7='We converted the original Brown Corpus tagset to the Universal tagset because the Universal tagset has fewer tags. More tags being used meaning sparser data will be generated. For example, a word might have several tags in the Brown tagset. Compared to the same word with fewer tags in the Universal tagset. The emission probability of the word and each of its tags will be lower. Therefore, the model using Brown tagset will be less confident while tagging the word and have a lower accuracy.'
a4full_vit=[[26.782208935674642, 27.175612695180405, 26.83747232781355, 25.69387747486122, 26.069930312266184, 25.33122669020975, 26.602772790690192, 26.7617409338053, 6.7157647918435055, 26.154747513228394, 28.346102791874564, 27.06281489027102], [34.01526253042612, 26.139704758753695, 34.54383920395947, 32.31894404945449, 34.25048137679512, 36.22833529220789, 27.617716596488677, 37.174460980126284, 34.96569792142533, 33.2167214529358, 30.719087535156927, 43.877716268423896], [52.96761801645391, 52.61690686578734, 53.24942252459456, 54.72356597513747, 52.53968502182914, 34.012187579823376, 51.358458806319945, 52.70225028680407, 55.14157828038837, 53.01419324341059, 54.12151433519213, 54.72321304758704], [64.0217433334945, 58.59624615156186, 64.37034195590547, 61.74080618135381, 67.52308363114038, 64.95730590909109, 44.11359964060699, 60.60376213192809, 62.6503191185711, 64.14971027238595, 61.85439770355395, 60.37766863378029]]
a4full_bp=[['PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON', 'PRON'], ['NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADJ'], ['DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']]
